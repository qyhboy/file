{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c:784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 784)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#orange\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'  #No logging TF\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from Generator import OmniglotGenerator\n",
    "from numpy import *;\n",
    "import random\n",
    "\n",
    "other_class=0\n",
    "nb_class =10+other_class\n",
    "input_size = 784\n",
    "img_size=(28,28)\n",
    "batch_size_om = 16\n",
    "nb_samples_per_class = 17\n",
    "train_samples_per_class=1\n",
    "\n",
    "train_samples=17\n",
    "mb_size = 64\n",
    "\n",
    "generator1 = OmniglotGenerator(data_folder='/home/pt/test1/data/omtrain', \\\n",
    "                               batch_size=batch_size_om, nb_samples=nb_class,\\\n",
    "                               nb_samples_per_class=nb_samples_per_class,\\\n",
    "                               max_rotation=0., max_shift=0.,img_size=img_size, max_iter=None,\\\n",
    "                               train_samples_per_class=train_samples_per_class)\n",
    "x_test,y_test=generator1.sample(nb_class)\n",
    "\n",
    "example_outputs = y_test.reshape(batch_size_om*nb_class*nb_samples_per_class,1)\n",
    "example_input=x_test.reshape(batch_size_om*nb_class*nb_samples_per_class,784)\n",
    "\n",
    "#test_others_input = np.zeros((batch_size_om *nb_samples_per_class*other_class,input_size))\n",
    "#test_others_outputs  =np.zeros((batch_size_om *nb_samples_per_class*other_class,nb_class))\n",
    "\n",
    "train_input=np.zeros((batch_size_om *train_samples*(nb_class-other_class),input_size))\n",
    "train_outputs=np.zeros((batch_size_om *train_samples*(nb_class-other_class),nb_class))\n",
    "jte=0\n",
    "jtr=0\n",
    "jto=0\n",
    "for k in range (nb_class):\n",
    "    for i in range (batch_size_om *nb_samples_per_class*nb_class):\n",
    "        if example_outputs[i]==k:\n",
    "            train_input[jtr]=example_input[i]\n",
    "            train_outputs[jtr][k]=1\n",
    "            jtr+=1\n",
    "            \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "def getbatchtrain(size):\n",
    "    batchtrain=np.zeros((size,input_size))\n",
    "    labeltrain=np.zeros((size,nb_class))\n",
    "    for i in range(size):\n",
    "        idx_1 = random.randint(0, batch_size_om *(nb_class-other_class)*train_samples-1)\n",
    "        batchtrain[i,:] = train_input[idx_1,:]\n",
    "        labeltrain[i,:]=train_outputs[idx_1,:]\n",
    "    return batchtrain,labeltrain\n",
    "\n",
    "def getbatchtrain2(size):\n",
    "    batchtrain=np.zeros((size,input_size))\n",
    "    labeltrain=np.zeros((size,nb_class))\n",
    "    batchtrain2=np.zeros((size,input_size))\n",
    "    labeltrain2=np.zeros((size,nb_class))\n",
    "    for i in range(size):\n",
    "        idx_1 = random.randint(0, batch_size_om *(nb_class-other_class)*train_samples-1)\n",
    "        jjj=idx_1/(batch_size_om*train_samples)\n",
    "        idx_2 = random.randint(int(jjj)*(batch_size_om*train_samples),int(jjj+1)*(batch_size_om*train_samples)-1)\n",
    "        batchtrain[i,:] = train_input[idx_1,:]\n",
    "        labeltrain[i,:]=train_outputs[idx_1,:]\n",
    "        batchtrain2[i,:] = train_input[idx_2,:]\n",
    "        labeltrain2[i,:]=train_outputs[idx_2,:]\n",
    "    return batchtrain,labeltrain,batchtrain2,labeltrain2\n",
    "\n",
    "z,_=getbatchtrain(64)\n",
    "#z,_=getbatchone(10)\n",
    "z.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CVAE2\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/home/pt/test1/ELM/data\", one_hot=True)\n",
    "# Parameters\n",
    "learning_rate2 = 0.001\n",
    "num_steps2 =160000\n",
    "batch_size2 = 64\n",
    "y_dim2=784\n",
    "y_dim22=10\n",
    "# Network Parameters\n",
    "image_dim2= 784 # MNIST images are 28x28 pixels\n",
    "hidden_dim20 = 512*2\n",
    "hidden_dim2 = 512\n",
    "hidden_dim22 = 512\n",
    "hidden_dim23=256\n",
    "hidden_dim24=128\n",
    "hidden_dim25=10\n",
    "latent_dim2 =2\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))\n",
    "\n",
    "def sample_c(m, n, ind=-1):\n",
    "    c = np.zeros([m,n])\n",
    "    for i in range(m):\n",
    "        if ind<0:\n",
    "            ind = np.random.randint(10)\n",
    "        c[i,ind] = 1\n",
    "    return c\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "weights2 = {\n",
    "    \n",
    "    'encoder_h1': tf.Variable(glorot_init([image_dim2+y_dim2+y_dim22, hidden_dim2])), \n",
    "    'z_mean': tf.Variable(glorot_init([hidden_dim2, latent_dim2])),\n",
    "    'z_std': tf.Variable(glorot_init([hidden_dim2, latent_dim2])),\n",
    "  \n",
    "    \n",
    "    'decoder_h1': tf.Variable(glorot_init([latent_dim2+y_dim2+y_dim22, hidden_dim22])),\n",
    "    'decoder_h2': tf.Variable(glorot_init([hidden_dim22, hidden_dim23])),\n",
    "    'decoder_h3': tf.Variable(glorot_init([hidden_dim23, hidden_dim22])),\n",
    "\n",
    "    'decoder_out': tf.Variable(glorot_init([hidden_dim22, image_dim2]))\n",
    "}\n",
    "biases2 = {\n",
    "\n",
    "    \n",
    "    'encoder_b1': tf.Variable(glorot_init([hidden_dim2])),\n",
    "    'z_mean': tf.Variable(glorot_init([latent_dim2])),\n",
    "    'z_std': tf.Variable(glorot_init([latent_dim2])),\n",
    "\n",
    "    \n",
    "    'decoder_b1': tf.Variable(glorot_init([hidden_dim22])),\n",
    "    'decoder_b2': tf.Variable(glorot_init([hidden_dim23])),\n",
    "    'decoder_b3': tf.Variable(glorot_init([hidden_dim22])),\n",
    "\n",
    "    'decoder_out': tf.Variable(glorot_init([image_dim2]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "input_image2 = tf.placeholder(tf.float32, shape=[None, image_dim2])\n",
    "\n",
    "\n",
    "y2 = tf.placeholder(tf.float32, shape=[None, y_dim2])\n",
    "y22 = tf.placeholder(tf.float32, shape=[None, y_dim22])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder2 = tf.matmul(tf.concat([input_image2,y2,y22], 1), weights2['encoder_h1']) + biases2['encoder_b1']\n",
    "encoder2 = tf.nn.tanh(encoder2)\n",
    "\n",
    "\n",
    "z_mean2 = tf.matmul(encoder2, weights2['z_mean']) + biases2['z_mean']\n",
    "z_std2 = tf.matmul(encoder2, weights2['z_std']) + biases2['z_std']\n",
    "\n",
    "# Sampler: Normal (gaussian) random distribution\n",
    "eps2 = tf.random_normal(tf.shape(z_std2), dtype=tf.float32, mean=0., stddev=1.0,\n",
    "                       name='epsilon')\n",
    "z2 = z_mean2 + tf.exp(z_std2 / 2) * eps2\n",
    "\n",
    "# Building the decoder (with scope to re-use these layers later)\n",
    "\n",
    "\n",
    "\n",
    "decoder2 = tf.matmul(tf.concat([z2,y2,y22], 1), weights2['decoder_h1']) + biases2['decoder_b1']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h2']) + biases2['decoder_b2']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h3']) + biases2['decoder_b3']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_out']) + biases2['decoder_out']\n",
    "decoder2 = tf.nn.sigmoid(decoder2)\n",
    "\n",
    "# Define VAE Loss\n",
    "def vae_loss2(x_reconstructed, x_true):\n",
    "    # Reconstruction loss\n",
    "    encode_decode_loss = x_true * tf.log(1e-10 + x_reconstructed) \\\n",
    "                         + (1 - x_true) * tf.log(1e-10 + 1 - x_reconstructed)\n",
    "    encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
    "    # KL Divergence loss\n",
    "    kl_div_loss = 1 + z_std2 - tf.square(z_mean2) - tf.exp(z_std2)\n",
    "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
    "    return tf.reduce_mean(encode_decode_loss + kl_div_loss)\n",
    "\n",
    "loss_op2 = vae_loss2(decoder2, input_image2)\n",
    "optimizer2 = tf.train.RMSPropOptimizer(learning_rate=learning_rate2)\n",
    "train_op2 = optimizer2.minimize(loss_op2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CVAE\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/home/pt/test1/ELM/data\", one_hot=True)\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps =80000\n",
    "batch_size = 64\n",
    "y_dim=10\n",
    "# Network Parameters\n",
    "image_dim = 784 # MNIST images are 28x28 pixels\n",
    "hidden_dim = 512\n",
    "hidden2_dim = 256\n",
    "hidden3_dim = 512*2\n",
    "latent_dim = 2\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))\n",
    "\n",
    "# Variables\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(glorot_init([image_dim+y_dim, hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'decoder_h1': tf.Variable(glorot_init([latent_dim+y_dim, hidden_dim])),\n",
    "    'decoder_h2': tf.Variable(glorot_init([hidden_dim, hidden2_dim])),\n",
    "    'decoder_h3': tf.Variable(glorot_init([hidden2_dim, hidden_dim])),\n",
    "    'decoder_out': tf.Variable(glorot_init([hidden_dim, image_dim]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([latent_dim])),\n",
    "    'decoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'decoder_b2': tf.Variable(glorot_init([hidden2_dim])),\n",
    "    'decoder_b3': tf.Variable(glorot_init([hidden_dim])),\n",
    "\n",
    "    'decoder_out': tf.Variable(glorot_init([image_dim]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "input_image = tf.placeholder(tf.float32, shape=[None, image_dim])\n",
    "noise_VAE=tf.placeholder(tf.float32, shape=[None, image_dim])\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "\n",
    "encoder = tf.matmul(tf.concat([input_image,y], 1), weights['encoder_h1']) + biases['encoder_b1']\n",
    "encoder = tf.nn.tanh(encoder)\n",
    "z_mean = tf.matmul(encoder, weights['z_mean']) + biases['z_mean']\n",
    "z_std = tf.matmul(encoder, weights['z_std']) + biases['z_std']\n",
    "\n",
    "# Sampler: Normal (gaussian) random distribution\n",
    "eps = tf.random_normal(tf.shape(z_std), dtype=tf.float32, mean=0., stddev=1.0,\n",
    "                       name='epsilon')\n",
    "z = z_mean + tf.exp(z_std / 2) * eps\n",
    "\n",
    "# Building the decoder (with scope to re-use these layers later)\n",
    "decoder = tf.matmul(tf.concat([z,y], 1), weights['decoder_h1']) + biases['decoder_b1']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_h2']) + biases['decoder_b2']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_h3']) + biases['decoder_b3']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "\n",
    "decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
    "decoder = tf.nn.sigmoid(decoder)\n",
    "\n",
    "# Define VAE Loss\n",
    "def vae_loss(x_reconstructed, x_true):\n",
    "    # Reconstruction loss\n",
    "    encode_decode_loss = x_true * tf.log(1e-10 + x_reconstructed) \\\n",
    "                         + (1 - x_true) * tf.log(1e-10 + 1 - x_reconstructed)\n",
    "    encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
    "    # KL Divergence loss\n",
    "    kl_div_loss = 1 + z_std - tf.square(z_mean) - tf.exp(z_std)\n",
    "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
    "    return tf.reduce_mean(encode_decode_loss + kl_div_loss)\n",
    "\n",
    "loss_op = vae_loss(decoder, input_image)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "#init = tf.global_variables_initializer()\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "#sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "#sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 605.902527\n",
      "Step 1000, Loss: 95.594437\n",
      "Step 2000, Loss: 74.974182\n",
      "Step 3000, Loss: 76.265404\n",
      "Step 4000, Loss: 71.905304\n",
      "Step 5000, Loss: 71.266678\n",
      "Step 6000, Loss: 74.701569\n",
      "Step 7000, Loss: 70.347214\n",
      "Step 8000, Loss: 75.134102\n",
      "Step 9000, Loss: 71.878983\n",
      "Step 10000, Loss: 108.799294\n",
      "Step 11000, Loss: 82.120041\n",
      "Step 12000, Loss: 81.283379\n",
      "Step 13000, Loss: 70.125099\n",
      "Step 14000, Loss: 73.837189\n",
      "Step 15000, Loss: 73.642151\n",
      "Step 16000, Loss: 78.132576\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "#saver.restore(sess, \"/home/pt/test1/omnist2/Utils/good-x1x2/drawmodel.ckpt\")\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    #batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "    #yin=np.zeros((batch_size,y_dim))\n",
    "    # Train\n",
    "    \n",
    "    #x_train1,_,x_train2,_=getbatchtraintest(64)\n",
    "    #x_train1,x_train2=getbatchtraintest(64)\n",
    "    #X_mb,y_mb= mnist.train.next_batch(64)\n",
    "    \n",
    "    #noiseG1_in=np.random.uniform(-0.2, 0.2, size=[300, 784])\n",
    "    #y_sample[:, 2] = 1\n",
    "    \n",
    "    x_train2,y_train2=getbatchtrain(64)\n",
    "\n",
    "    #Z_sample = sample_Z(64, Z_dim1)\n",
    "    #samples = sess.run(G_sample, feed_dict={Z: Z_sample, y1:y_train2})\n",
    "    feed_dict = {input_image: x_train2,y:y_train2}\n",
    "    _, l = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
    "    \n",
    "        \n",
    "  \n",
    "    if i % 1000 == 0 or i == 1:\n",
    "        print('Step %i, Loss: %f' % (i, l))\n",
    "        \n",
    "\n",
    "noise_input = tf.placeholder(tf.float32, shape=[None, latent_dim])\n",
    "#y = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "# Rebuild the decoder to create image from noise\n",
    "decoder = tf.matmul(tf.concat([noise_input,y], 1), weights['decoder_h1']) + biases['decoder_b1']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_h2']) + biases['decoder_b2']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_h3']) + biases['decoder_b3']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
    "decoder = tf.nn.sigmoid(decoder)\n",
    "n = 15\n",
    "noise_dim=2\n",
    "for i in range(1, num_steps2+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    #batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "    #yin=np.zeros((batch_size,y_dim))\n",
    "    # Train\n",
    "    \n",
    "    #x_train1,_,x_train2,_=getbatchtraintest(64)\n",
    "    #x_train1,x_train2=getbatchtraintest(64)\n",
    "    #X_mb,y_mb= mnist.train.next_batch(64)\n",
    "    yset=sample_c(32,y_dim22)\n",
    "    #noiseG1_in=np.random.uniform(-0.25, 0.25, size=[64, 784])\n",
    "    #y_sample[:, 2] = 1\n",
    "    ztest = np.random.uniform(-1., 1., size=[32, noise_dim])\n",
    "    x_train2,y_train2=getbatchtrain(32)\n",
    "    g = sess.run(decoder, feed_dict={noise_input: ztest,y:y_train2 })\n",
    "    #Z_sample = sample_Z(64, Z_dim1)\n",
    "    #samples = sess.run(G_sample, feed_dict={Z: Z_sample, y1:y_train2})\n",
    "    feed_dict = {input_image2: x_train2,y2:g,y22:yset}\n",
    "    _, l = sess.run([train_op2, loss_op2], feed_dict=feed_dict)\n",
    "    \n",
    "    if i%100:\n",
    "        #ztest = np.random.uniform(-1., 1., size=[64, noise_dim])\n",
    "        yset=sample_c(32,y_dim22)\n",
    "        X_mb,y_mb,X_mb2,y_mb2=getbatchtrain2(32)\n",
    " \n",
    "        feed_dict = {input_image2: X_mb,y2:X_mb2,y22:yset}\n",
    "        _, l = sess.run([train_op2, loss_op2], feed_dict=feed_dict)\n",
    "        \n",
    "    \n",
    "    if i % 1000 == 0 or i == 1:\n",
    "       \n",
    "        print('Step %i, Loss: %f' % (i, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
