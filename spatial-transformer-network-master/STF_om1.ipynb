{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#orange\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'  #No logging TF\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from Generator import OmniglotGenerator\n",
    "from numpy import *;\n",
    "import random\n",
    "\n",
    "other_class=0\n",
    "nb_class =10+other_class\n",
    "input_size = 784\n",
    "img_size=(28,28)\n",
    "batch_size_om = 16\n",
    "nb_samples_per_class = 17\n",
    "train_samples_per_class=1\n",
    "\n",
    "train_samples=17\n",
    "mb_size = 64\n",
    "\n",
    "generator1 = OmniglotGenerator(data_folder='/home/pt/test1/data/omtrain', \\\n",
    "                               batch_size=batch_size_om, nb_samples=nb_class,\\\n",
    "                               nb_samples_per_class=nb_samples_per_class,\\\n",
    "                               max_rotation=0., max_shift=0.,img_size=img_size, max_iter=None,\\\n",
    "                               train_samples_per_class=train_samples_per_class)\n",
    "x_test,y_test=generator1.sample(nb_class)\n",
    "\n",
    "example_outputs = y_test.reshape(batch_size_om*nb_class*nb_samples_per_class,1)\n",
    "example_input=x_test.reshape(batch_size_om*nb_class*nb_samples_per_class,784)\n",
    "\n",
    "#test_others_input = np.zeros((batch_size_om *nb_samples_per_class*other_class,input_size))\n",
    "#test_others_outputs  =np.zeros((batch_size_om *nb_samples_per_class*other_class,nb_class))\n",
    "\n",
    "train_input=np.zeros((batch_size_om *train_samples*(nb_class-other_class),input_size))\n",
    "train_outputs=np.zeros((batch_size_om *train_samples*(nb_class-other_class),nb_class))\n",
    "jte=0\n",
    "jtr=0\n",
    "jto=0\n",
    "for k in range (nb_class):\n",
    "    for i in range (batch_size_om *nb_samples_per_class*nb_class):\n",
    "        if example_outputs[i]==k:\n",
    "            train_input[jtr]=example_input[i]\n",
    "            train_outputs[jtr][k]=1\n",
    "            jtr+=1\n",
    "            \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "def getbatchtrain(size):\n",
    "    batchtrain=np.zeros((size,input_size))\n",
    "    labeltrain=np.zeros((size,nb_class))\n",
    "    for i in range(size):\n",
    "        idx_1 = random.randint(0, batch_size_om *(nb_class-other_class)*train_samples-1)\n",
    "        batchtrain[i,:] = train_input[idx_1,:]\n",
    "        labeltrain[i,:]=train_outputs[idx_1,:]\n",
    "    return batchtrain,labeltrain\n",
    "\n",
    "def getbatchtrain2(size):\n",
    "    batchtrain=np.zeros((size,input_size))\n",
    "    labeltrain=np.zeros((size,nb_class))\n",
    "    batchtrain2=np.zeros((size,input_size))\n",
    "    labeltrain2=np.zeros((size,nb_class))\n",
    "    for i in range(size):\n",
    "        idx_1 = random.randint(0, batch_size_om *(nb_class-other_class)*train_samples-1)\n",
    "        jjj=idx_1/(batch_size_om*train_samples)\n",
    "        idx_2 = random.randint(int(jjj)*(batch_size_om*train_samples),int(jjj+1)*(batch_size_om*train_samples)-1)\n",
    "        batchtrain[i,:] = train_input[idx_1,:]\n",
    "        labeltrain[i,:]=train_outputs[idx_1,:]\n",
    "        batchtrain2[i,:] = train_input[idx_2,:]\n",
    "        labeltrain2[i,:]=train_outputs[idx_2,:]\n",
    "    return batchtrain,labeltrain,batchtrain2,labeltrain2\n",
    "\n",
    "def getbatchtrain3(size):\n",
    "    batchtrain=np.zeros((size,input_size))\n",
    "    labeltrain=np.zeros((size,nb_class))\n",
    "    batchtrain2=np.zeros((size,input_size))\n",
    "    labeltrain2=np.zeros((size,nb_class))\n",
    "    batchtrain3=np.zeros((size,input_size))\n",
    "    labeltrain3=np.zeros((size,nb_class))\n",
    "    for i in range(size):\n",
    "        idx_1 = random.randint(0, batch_size_om *(nb_class-other_class)*train_samples-1)\n",
    "        jjj=idx_1/(batch_size_om*train_samples)\n",
    "        idx_2 = random.randint(int(jjj)*(batch_size_om*train_samples),int(jjj+1)*(batch_size_om*train_samples)-1)\n",
    "        idx_3 = random.randint(int(jjj)*(batch_size_om*train_samples),int(jjj+1)*(batch_size_om*train_samples)-1)\n",
    "        batchtrain[i,:] = train_input[idx_1,:]\n",
    "        labeltrain[i,:]=train_outputs[idx_1,:]\n",
    "        batchtrain2[i,:] = train_input[idx_2,:]\n",
    "        labeltrain2[i,:]=train_outputs[idx_2,:]\n",
    "        batchtrain3[i,:] = train_input[idx_3,:]\n",
    "        labeltrain3[i,:]=train_outputs[idx_3,:]\n",
    "    return batchtrain,labeltrain,batchtrain2,labeltrain2,batchtrain3,labeltrain3\n",
    "\n",
    "z,_=getbatchtrain(64)\n",
    "#z,_=getbatchone(10)\n",
    "z.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CVAE2\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/pt/test1/data/mnist\", one_hot=True)\n",
    "# Parameters\n",
    "learning_rate2 = 0.001\n",
    "num_steps=160000\n",
    "batch_size= 32\n",
    "y_dim2=784\n",
    "y_dim22=10\n",
    "# Network Parameters\n",
    "image_dim2= 784 # MNIST images are 28x28 pixels\n",
    "hidden_dim20 = 512*2\n",
    "hidden_dim2 = 512\n",
    "hidden_dim22 = 512\n",
    "hidden_dim23=256\n",
    "hidden_dim24=128\n",
    "hidden_dim25=10\n",
    "\n",
    "latent_dim2 =20\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))\n",
    "\n",
    "def sample_c(m, n, ind=-1):\n",
    "    c = np.zeros([m,n])\n",
    "    for i in range(m):\n",
    "        if ind<0:\n",
    "            ind = np.random.randint(10)\n",
    "        c[i,ind] = 1\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "# Variables\n",
    "weights2 = {\n",
    "     'encoder_h01': tf.Variable(glorot_init([y_dim2, hidden_dim2])), \n",
    "    'encoder_h02': tf.Variable(glorot_init([hidden_dim2, hidden_dim23])), \n",
    "    'encoder_h03': tf.Variable(glorot_init([hidden_dim23, hidden_dim24])), \n",
    "\n",
    "    'encoder_h1': tf.Variable(glorot_init([image_dim2+hidden_dim24+y_dim22, hidden_dim2])), \n",
    "    'z_mean': tf.Variable(glorot_init([hidden_dim2, latent_dim2])),\n",
    "    'z_std': tf.Variable(glorot_init([hidden_dim2, latent_dim2])),\n",
    "    'decoder_h01': tf.Variable(glorot_init([y_dim2, hidden_dim2])),\n",
    "    'decoder_h02': tf.Variable(glorot_init([hidden_dim2, hidden_dim23])),\n",
    "    'decoder_h03': tf.Variable(glorot_init([hidden_dim23, hidden_dim24])),\n",
    "   \n",
    "    'decoder_h1': tf.Variable(glorot_init([latent_dim2+hidden_dim24+y_dim22, hidden_dim22])),\n",
    "    'decoder_h2': tf.Variable(glorot_init([hidden_dim22, hidden_dim23])),\n",
    "    'decoder_h3': tf.Variable(glorot_init([hidden_dim23, hidden_dim22])),\n",
    "\n",
    "    'decoder_out': tf.Variable(glorot_init([hidden_dim22, image_dim2]))\n",
    "}\n",
    "biases2 = {\n",
    "    'encoder_b01': tf.Variable(glorot_init([hidden_dim2])),\n",
    "    'encoder_b02': tf.Variable(glorot_init([hidden_dim23])),\n",
    "    'encoder_b03': tf.Variable(glorot_init([hidden_dim24])),\n",
    "\n",
    "    'encoder_b1': tf.Variable(glorot_init([hidden_dim2])),\n",
    "    'z_mean': tf.Variable(glorot_init([latent_dim2])),\n",
    "    'z_std': tf.Variable(glorot_init([latent_dim2])),\n",
    "    'decoder_b01': tf.Variable(glorot_init([hidden_dim2])),\n",
    "    'decoder_b02': tf.Variable(glorot_init([hidden_dim23])),\n",
    "    'decoder_b03': tf.Variable(glorot_init([hidden_dim24])),\n",
    "\n",
    "    'decoder_b1': tf.Variable(glorot_init([hidden_dim22])),\n",
    "    'decoder_b2': tf.Variable(glorot_init([hidden_dim23])),\n",
    "    'decoder_b3': tf.Variable(glorot_init([hidden_dim22])),\n",
    "\n",
    "    'decoder_out': tf.Variable(glorot_init([image_dim2]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "input_image2 = tf.placeholder(tf.float32, shape=[None, image_dim2])\n",
    "\n",
    "\n",
    "y2 = tf.placeholder(tf.float32, shape=[None, y_dim2])\n",
    "y22 = tf.placeholder(tf.float32, shape=[None, y_dim22])\n",
    "\n",
    "encoder2 = tf.matmul(y2, weights2['encoder_h01']) + biases2['encoder_b01']\n",
    "encoder2 = tf.nn.sigmoid(encoder2)\n",
    "encoder2 = tf.matmul(encoder2, weights2['encoder_h02']) + biases2['encoder_b02']\n",
    "encoder2 = tf.nn.sigmoid(encoder2)\n",
    "encoder2 = tf.matmul(encoder2, weights2['encoder_h03']) + biases2['encoder_b03']\n",
    "encoder2 = tf.nn.sigmoid(encoder2)\n",
    "\n",
    "\n",
    "encoder2 = tf.matmul(tf.concat([input_image2,encoder2,y22], 1), weights2['encoder_h1']) + biases2['encoder_b1']\n",
    "encoder2 = tf.nn.tanh(encoder2)\n",
    "\n",
    "\n",
    "z_mean2 = tf.matmul(encoder2, weights2['z_mean']) + biases2['z_mean']\n",
    "z_std2 = tf.matmul(encoder2, weights2['z_std']) + biases2['z_std']\n",
    "\n",
    "# Sampler: Normal (gaussian) random distribution\n",
    "eps2 = tf.random_normal(tf.shape(z_std2), dtype=tf.float32, mean=0., stddev=1.0,\n",
    "                       name='epsilon')\n",
    "z2 = z_mean2 + tf.exp(z_std2 / 2) * eps2\n",
    "\n",
    "# Building the decoder (with scope to re-use these layers later)\n",
    "decoder2 = tf.matmul(y2, weights2['decoder_h01']) + biases2['decoder_b01']\n",
    "decoder2 = tf.nn.sigmoid(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h02']) + biases2['decoder_b02']\n",
    "decoder2 = tf.nn.sigmoid(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h03']) + biases2['decoder_b03']\n",
    "decoder2 = tf.nn.sigmoid(decoder2)\n",
    "\n",
    "\n",
    "decoder2 = tf.matmul(tf.concat([z2,decoder2,y22], 1), weights2['decoder_h1']) + biases2['decoder_b1']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h2']) + biases2['decoder_b2']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_h3']) + biases2['decoder_b3']\n",
    "decoder2 = tf.nn.tanh(decoder2)\n",
    "\n",
    "decoder2 = tf.matmul(decoder2, weights2['decoder_out']) + biases2['decoder_out']\n",
    "decoder2 = tf.nn.sigmoid(decoder2)\n",
    "\n",
    "# Define VAE Loss\n",
    "def vae_loss2(x_reconstructed, x_true):\n",
    "    # Reconstruction loss\n",
    "    encode_decode_loss = x_true * tf.log(1e-10 + x_reconstructed) \\\n",
    "                         + (1 - x_true) * tf.log(1e-10 + 1 - x_reconstructed)\n",
    "    encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
    "    # KL Divergence loss\n",
    "    kl_div_loss = 1 + z_std2 - tf.square(z_mean2) - tf.exp(z_std2)\n",
    "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
    "    return tf.reduce_mean(encode_decode_loss + kl_div_loss)\n",
    "\n",
    "loss_op2 = vae_loss2(decoder2, input_image2)\n",
    "optimizer2 = tf.train.RMSPropOptimizer(learning_rate=learning_rate2)\n",
    "train_op2 = optimizer2.minimize(loss_op2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "noise_dim=20\n",
    "for i in range(1, num_steps2+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    #batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "    #yin=np.zeros((batch_size,y_dim))\n",
    "    # Train\n",
    "    \n",
    "    #x_train1,_,x_train2,_=getbatchtraintest(64)\n",
    "    #x_train1,x_train2=getbatchtraintest(64)\n",
    "    #X_mb,y_mb= mnist.train.next_batch(64)\n",
    " \n",
    "    \n",
    "    if (1):\n",
    "        #ztest = np.random.uniform(-1., 1., size=[64, noise_dim])\n",
    "        yset=sample_c(32,y_dim22)\n",
    "        X_mb,y_mb,X_mb2,y_mb2=getbatchtrain2(32)\n",
    " \n",
    "        feed_dict = {input_image2: X_mb,y2:X_mb2,y22:yset}\n",
    "        _, l = sess.run([train_op2, loss_op2], feed_dict=feed_dict)\n",
    "        \n",
    "    \n",
    "    if i % 1000 == 0 or i == 1:\n",
    "\n",
    "        print('Step %i, Loss: %f' % (i, l))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
